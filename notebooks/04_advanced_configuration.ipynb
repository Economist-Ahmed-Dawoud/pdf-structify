{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4: Advanced Configuration\n",
    "\n",
    "This notebook covers advanced features for power users.\n",
    "\n",
    "**What you'll learn:**\n",
    "- Schema save/load for resume capability\n",
    "- Using different models for detection vs extraction\n",
    "- Extraction sampling for cost control and testing\n",
    "- Checkpoint and resume functionality\n",
    "- API retry and error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "from structify import Pipeline, Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Schema Save/Load (Resume Capability)\n",
    "\n",
    "Save your detected schema to skip detection in future runs. This is useful when:\n",
    "- You want to extract from new documents using the same schema\n",
    "- You want to resume extraction without re-running detection\n",
    "- You want to share schemas between team members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Saving a Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detection and save the schema\n",
    "pipeline = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    detection_mode=\"moderate\",\n",
    ")\n",
    "\n",
    "# Fit detects the schema\n",
    "pipeline.fit(\"documents/\")\n",
    "\n",
    "# Save schema to JSON or YAML\n",
    "pipeline.save_schema(\"my_schema.json\")   # JSON format\n",
    "# pipeline.save_schema(\"my_schema.yaml\")  # YAML format also supported\n",
    "\n",
    "print(\"Schema saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loading a Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load schema in pipeline constructor\n",
    "pipeline = Pipeline(schema=\"my_schema.json\")\n",
    "\n",
    "# fit() is instant - no detection needed!\n",
    "pipeline.fit(\"documents/\")\n",
    "\n",
    "# Extract data\n",
    "results = pipeline.transform(\"documents/\")\n",
    "print(f\"Extracted {len(results)} records using saved schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Load and inspect schema before using\n",
    "schema = Schema.load(\"my_schema.json\")\n",
    "\n",
    "print(f\"Schema: {schema.name}\")\n",
    "print(f\"Fields: {len(schema.fields)}\")\n",
    "for field in schema.fields:\n",
    "    print(f\"  - {field.name}: {field.type.value}\")\n",
    "\n",
    "# Use with pipeline\n",
    "pipeline = Pipeline(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Schema File Format\n",
    "\n",
    "The saved schema file looks like this (JSON):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"detected_schema\",\n",
    "  \"description\": \"Automatically detected schema for research paper (findings)\",\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"name\": \"estimate_value\",\n",
    "      \"type\": \"float\",\n",
    "      \"description\": \"The coefficient or effect size\",\n",
    "      \"required\": true\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"methodology\",\n",
    "      \"type\": \"categorical\",\n",
    "      \"description\": \"Research methodology used\",\n",
    "      \"options\": [\"DID\", \"IV\", \"RDD\", \"OLS\", \"FE\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection\n",
    "\n",
    "Use different models for schema detection vs data extraction:\n",
    "\n",
    "- **Detection**: Use a fast model (cheaper, quicker)\n",
    "- **Extraction**: Use a powerful model (more accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fast model for detection, powerful model for extraction\n",
    "pipeline = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    detection_model=\"gemini-2.0-flash\",   # Fast and cheap for detection\n",
    "    extraction_model=\"gemini-2.5-pro\",    # Powerful and accurate for extraction\n",
    ")\n",
    "\n",
    "results = pipeline.fit_transform(\"documents/\")\n",
    "print(f\"Extracted {len(results)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Models\n",
    "\n",
    "| Model | Speed | Cost | Accuracy | Best For |\n",
    "|-------|-------|------|----------|----------|\n",
    "| `gemini-2.0-flash` | Fast | Low | Good | Detection, quick tests |\n",
    "| `gemini-2.5-flash` | Fast | Low | Better | Detection |\n",
    "| `gemini-2.5-pro` | Slower | Higher | Best | Extraction |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Same fast model for both (cost-effective)\n",
    "pipeline_fast = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    detection_model=\"gemini-2.0-flash\",\n",
    "    extraction_model=\"gemini-2.0-flash\",\n",
    ")\n",
    "\n",
    "# Example: Powerful model for both (maximum accuracy)\n",
    "pipeline_accurate = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    detection_model=\"gemini-2.5-pro\",\n",
    "    extraction_model=\"gemini-2.5-pro\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extraction Sampling\n",
    "\n",
    "Process only a subset of documents for:\n",
    "- Quick testing and validation\n",
    "- Cost control\n",
    "- Initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Sample by Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from only 20% of documents\n",
    "pipeline = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    extraction_sample_ratio=0.2,  # 20% of files\n",
    "    seed=42,                      # Reproducible sampling\n",
    ")\n",
    "\n",
    "results = pipeline.fit_transform(\"documents/\")\n",
    "print(f\"Extracted from ~20% of documents: {len(results)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Sample with Maximum Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from at most 50 documents\n",
    "pipeline = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    extraction_max_samples=50,  # No more than 50 files\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "results = pipeline.fit_transform(\"documents/\")\n",
    "print(f\"Extracted from max 50 documents: {len(results)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Combined Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from 30% of documents, but no more than 100\n",
    "pipeline = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    extraction_sample_ratio=0.3,   # 30%\n",
    "    extraction_max_samples=100,    # Cap at 100\n",
    "    seed=42,                       # Reproducible\n",
    ")\n",
    "\n",
    "results = pipeline.fit_transform(\"documents/\")\n",
    "print(f\"Extracted records: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reproducible Sampling\n",
    "\n",
    "Use `seed` to get the same sample every time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same seed = same documents selected\n",
    "pipeline1 = Pipeline(extraction_sample_ratio=0.2, seed=42)\n",
    "pipeline2 = Pipeline(extraction_sample_ratio=0.2, seed=42)\n",
    "\n",
    "# Both will process the exact same subset of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Checkpoint and Resume\n",
    "\n",
    "Never lose progress! pdf-structify saves checkpoints automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable checkpoints (enabled by default)\n",
    "pipeline = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    enable_checkpoints=True,\n",
    "    state_dir=\".structify_state\",  # Where to save state\n",
    ")\n",
    "\n",
    "# Start extraction\n",
    "results = pipeline.fit_transform(\"documents/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Resume After Interruption\n",
    "\n",
    "If interrupted (Ctrl+C, crash, power outage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run again - it resumes automatically!\n",
    "pipeline = Pipeline.resume(\"documents/\")\n",
    "results = pipeline.transform(\"documents/\")\n",
    "\n",
    "print(f\"Resumed and completed: {len(results)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Force Restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh, ignoring existing checkpoints\n",
    "pipeline = Pipeline(purpose=\"findings\")\n",
    "results = pipeline.fit_transform(\"documents/\", force_restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. API Retry and Error Handling\n",
    "\n",
    "pdf-structify handles API errors automatically:\n",
    "\n",
    "- **API errors**: 1 retry with 2-second delay\n",
    "- **Rate limits**: Automatic backoff and retry\n",
    "- **Timeouts**: Retry with increasing delays\n",
    "\n",
    "No configuration needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retry is automatic - just use the pipeline normally\n",
    "pipeline = Pipeline(purpose=\"findings\")\n",
    "results = pipeline.fit_transform(\"documents/\")\n",
    "\n",
    "# If API errors occur, you'll see:\n",
    "# WARNING: API error: [error]. Retrying in 2 seconds..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Advanced Example\n",
    "\n",
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from structify import Pipeline, Schema\n",
    "\n",
    "# Full-featured pipeline configuration\n",
    "pipeline = Pipeline(\n",
    "    # Purpose and detection\n",
    "    purpose=\"findings\",\n",
    "    detection_mode=\"moderate\",\n",
    "    \n",
    "    # Model selection\n",
    "    detection_model=\"gemini-2.0-flash\",    # Fast for detection\n",
    "    extraction_model=\"gemini-2.5-pro\",     # Accurate for extraction\n",
    "    \n",
    "    # Sampling\n",
    "    extraction_sample_ratio=0.5,           # 50% of documents\n",
    "    extraction_max_samples=100,            # Max 100 documents\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=42,\n",
    "    \n",
    "    # Checkpointing\n",
    "    enable_checkpoints=True,\n",
    "    state_dir=\".my_extraction_state\",\n",
    "    \n",
    "    # Output processing\n",
    "    deduplicate=True,\n",
    "    pages_per_chunk=10,\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline.fit(\"documents/\")\n",
    "\n",
    "# Save schema for future use\n",
    "pipeline.save_schema(\"production_schema.json\")\n",
    "\n",
    "# Extract data\n",
    "results = pipeline.transform(\"documents/\")\n",
    "\n",
    "# Export\n",
    "results.to_csv(\"extracted_data.csv\", index=False)\n",
    "\n",
    "print(f\"\\nExtraction complete!\")\n",
    "print(f\"Records: {len(results)}\")\n",
    "print(f\"Schema saved to: production_schema.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Workflow for Production\n",
    "\n",
    "Recommended workflow for large-scale extractions:\n",
    "\n",
    "1. **Test on sample**: Use sampling to validate schema\n",
    "2. **Save schema**: Keep the schema for consistency\n",
    "3. **Full extraction**: Run on all documents with saved schema\n",
    "4. **Use checkpoints**: Enable for safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Test on 10% sample\n",
    "test_pipeline = Pipeline(\n",
    "    purpose=\"findings\",\n",
    "    extraction_sample_ratio=0.1,\n",
    "    seed=42,\n",
    ")\n",
    "test_results = test_pipeline.fit_transform(\"documents/\")\n",
    "\n",
    "# Inspect results...\n",
    "print(f\"Test extraction: {len(test_results)} records\")\n",
    "print(test_results.head())\n",
    "\n",
    "# Step 2: Save schema if satisfied\n",
    "test_pipeline.save_schema(\"validated_schema.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Full extraction with saved schema\n",
    "production_pipeline = Pipeline(\n",
    "    schema=\"validated_schema.json\",  # Skip detection\n",
    "    extraction_model=\"gemini-2.5-pro\",\n",
    "    enable_checkpoints=True,\n",
    ")\n",
    "\n",
    "production_pipeline.fit(\"documents/\")  # Instant - uses saved schema\n",
    "final_results = production_pipeline.transform(\"documents/\")\n",
    "\n",
    "final_results.to_csv(\"final_output.csv\", index=False)\n",
    "print(f\"Full extraction: {len(final_results)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "- ✅ Schema save/load for resume capability\n",
    "- ✅ Model selection (detection vs extraction)\n",
    "- ✅ Extraction sampling for cost control\n",
    "- ✅ Checkpoint and resume functionality\n",
    "- ✅ Automatic API retry handling\n",
    "- ✅ Production workflow best practices\n",
    "\n",
    "**Next:** Tutorial 5 - Custom Schema Building"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
